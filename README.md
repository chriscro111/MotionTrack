### Chris Cronin, Andrew Gardner
### Fall 2015 ECE 5554/4984 Computer Vision: Class Project
### Virginia Tech
  
# Abstract
Gesture control is an interesting feature that has been tested in many modern applications. The motivation behind our project was to accomplish the first step of gesture control and be able to track objects through a video. To simplify the task we tracked bright yellow balls through a video using the LAB colorspace. We then used the average of the positions of the pixels identified to obtain the center of each individual ball and used proximity to ensure the same center stayed with the same ball from frame to frame. We developed a system in MATLAB that was capable of taking a video file, parsing it into frames, identify the centers of the balls, and track those centers throughout. There are some slight issues where if a ball is initially in the video then leaves and re-enters it will not be tracked and the centers will get mixed up.

# Teaser Figure
![Balls with centers tracked](images/teaser.png)
Balls used as markers shown with the centers identified

# Introduction
Something seen in many sci-fi films is the use of computers through gesture control. The first step to be able to move towards this feature is the ability to track the object being used to make the gesture. For our project we solve a simplified problem and track the centers of yellow balls through a video. In order to do this we do some computations on each individual frame of the video which we get by splitting the video into frames within MATLAB. We also use the the LAB colorspace instead of the RGB colorspace since the balls we use are significantly brighter than the rest of the content in each frame. This is highly dependent on the lighting of the setting the motion tracking is occurring in so our system does assume that the lighting will remain the same throughout the video and does require to some initial user input in order to get the appropriate bounds so only the balls will be identified their brightness. With the successful tracking of the markers the data gathered from our system could be used to implement some sort of gesture control system based on the path of the markers through the video. 

# Approach
Our approach to solve the problem of tracking objects in a video was to break up the problem into smaller tasks and accomplish those and put the pieces of the individual problems together in a system to solve the overall problem. The first problem that we went about handling was creating a binary image with the markers identified and labeled. To do this we initially only worked with a single image with the markers in it until we got the results we wanted. Since the markers we were using were yellow they were much brighter than the rest of the setting we were using the system in so we decided to use the brightness of the LAB color space to separate out the markers. We found a good set of bounds for the brightness values that would clearly separate out the balls, but when we applied these bounds to all the frames of a video we quickly realized that there was an issue with the quality of the blobs that were separated out. In order to resolve this and make the blobs more uniform we eroded and dilated the image. This made the blobs more uniform so we would not run into issues with finding the centers. At this stage we also realized we needed some input to determine what bounds should be used to separate the blobs by brightness since these bounds changed depending on lighting. To do this we used a threshold tool found online. CHRIS TALK ABOUT THE LABELS HERE ALSO CITE WHERE YOU FOUND THE THRESHOLD TOOOOOOOOOOOOLLLLLLLLLLLLLLL LOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLL. Once we had each blob labeled we found the center for each blob. To do this we used a pretty simple approach and simply took the average of all the coordinates of labeled blob. These average centers were then added to a list depending on their proximity to the previous centers. We initially did not look at the proximity of the centers to the previous centers, but quickly realized there was no guarantee of each blob receiving the same label in each frame when centers began showing up in the incorrect lists. Once we had the lists we simply produced an output that displayed each frame with the center marked on each marker using the lists of centers.  

# Experiments and results
Provide details about the experimental set up (number of images/videos, number of datasets you experimented with, train/test split if you used machine learning algorithms, etc.). Describe the evaluation metrics you used to evaluate how well your approach is working. Include clear figures and tables, as well as illustrative qualitative examples if appropriate. Be sure to include obvious baselines to see if your approach is doing better than a naive approach (e.g. for classification accuracy, how well would a classifier do that made random decisions?). Also discuss any parameters of your algorithms, and tell us how you set the values of those parameters. You can also show us how the performance varies as you change those parameter values. Be sure to discuss any trends you see in your results, and explain why these trends make sense. Are the results as expected? Why?

# Qualitative results
Show several visual examples of inputs/outputs of your system (success cases and failures) that help us better understand your approach. 
